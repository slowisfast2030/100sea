{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark-submit 用户打包 Spark 应用程序并部署到 Spark 支持的集群管理气上，命令语法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit [options] <python file> [app arguments]\n",
    "```\n",
    "app arguments 是传递给应用程序的参数，常用的命令行参数如下所示：\n",
    "* –master: 设置主节点 URL 的参数。支持：\n",
    "    * local： 本地机器。\n",
    "    * spark://host:port：远程 Spark 单机集群。\n",
    "    * yarn：yarn 集群\n",
    "* –deploy-mode：允许选择是否在本地（使用 client 选项）启动 Spark 驱动程序，或者在集群内（使用 cluster 选项）的其中一台工作机器上启动。默认值是 client。\n",
    "* –name：应用程序名称，也可在程序内设置。\n",
    "* –py-files：.py, .egg 或者 .zip 文件的逗号分隔列表，包括 Python 应用程序。这些文件将分发给每个执行节点。\n",
    "* –files：逗号分隔的文件列表，这些文件将分发给每个执行节点。\n",
    "* –conf：动态地改变应用程序的配置。\n",
    "* –driver-memory：指定应用程序在驱动节点上分配多少内存的参数，类似与 10000M， 2G。默认值是 1024M。\n",
    "* –executor-memory：指定每个执行节点上为应用程序分配的内存，默认 1G。\n",
    "* –num-executors：指定执行器节点数。\n",
    "* –help：展示帮助信息和退出。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下均是在 yarn 集群提交的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、默认设置: 会将所有日志和系统输出结果输出到 spark-submit 的 client 上\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             code1.py\n",
    "```\n",
    "\n",
    "code1.py\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Test_Code1') \\\n",
    "                            .enableHiveSupport() \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark.sql(\"select count(*) from default.test_table\").show()\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、设置 Executor 的日志级别，Executor 执行的细节（WARN 以下级别的日志）不会输出到 client 中\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             code2.py\n",
    "```\n",
    "code2.py\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Test_Code1') \\\n",
    "                            .enableHiveSupport() \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "spark.sql(\"select count(*) from default.test_table\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、使用 cluster 模式\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             --deploy-mode cluster \\\n",
    "             code1.py\n",
    "```\n",
    "–deploy-mode 可选 cluster 或 client，cluster 模式下，在 spark-submit 的 client 服务器上不会输出日志和系统输出，仅输出如下语句。只能在 Hadoop 集群上才能看到执行细节和输出\n",
    "\n",
    ">2019-09-06 00:00:00 INFO  Client:54 - Application report for application_1556516318747_25363 (state: RUNNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、自定义依赖的模块或读取文件\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             --files file1.txt \\\n",
    "             --py-files code4.py \\\n",
    "             code3.py \n",
    "```\n",
    "code3.py\n",
    "```\n",
    "from code4 import code4func\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Test_Code1') \\\n",
    "                            .enableHiveSupport() \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "table = code4func()\n",
    "with open(\"file1.txt\", 'rt') as rf:\n",
    "    db = rf.readline().strip()\n",
    "\n",
    "spark.sql(\"select count(*) from {}.{}\".format(db, table)).show()\n",
    "```\n",
    "code4.py\n",
    "```\n",
    "def code4func():\n",
    "    return \"test_table\"\n",
    "\n",
    "```\n",
    "file1.txt\n",
    "```\n",
    "default\n",
    "\n",
    "```\n",
    "\n",
    "自定义的 package 可以打包成 egg 文件上传(该部分代码参考 《PySpark 实战》P:178)。例如有一个自定义创建的 package：\n",
    "```\n",
    "additionalCode/\n",
    "├── setup.py\n",
    "└── utilities\n",
    "    ├── __init__.py\n",
    "    ├── base.py\n",
    "    ├── converters\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── distance.py\n",
    "    └── geoCalc.py\n",
    "\n",
    "```\n",
    "创建一个 egg 文件：\n",
    "\n",
    "```\n",
    "python setup.py bdist_egg\n",
    "```\n",
    "生成了 dist 文件夹下的 PySparkUtilities-0.1.dev0-py3.6.egg 文件\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             --py-files additionalCode/dist/PySparkUtilities-0.1.dev0-py3.6.egg \\\n",
    "             calculatingGeoDistance.py\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、配置集群资源\n",
    "\n",
    "当执行的 job 需要更多资源时，可以自定义配置使用的资源。\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             --driver-memory 15g \\\n",
    "             --num-executors 10 \\\n",
    "             --executor-cores 4 \\\n",
    "             --executor-memory 15G \\\n",
    "             --conf spark.executor.memoryOverhead=15G \\\n",
    "             code1.py\n",
    "```\n",
    "或在程序内设置\n",
    "```\n",
    "spark-submit code5.py\n",
    "```\n",
    "code5.py\n",
    "```\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf1 = pyspark.SparkConf().setAll([\n",
    "            ('spark.executor.memory', '15g'),\n",
    "            ('spark.executor.memoryOverhead', '16g'),\n",
    "            ('spark.executor.cores', '4'),\n",
    "            ('spark.num.executors', '10'),\n",
    "            ('spark.driver.memory', '16g')])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('Test_Code1') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .config(conf=conf1) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sql(\"select count(*) from default.test_table\").show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、使用 Python 虚拟环境\n",
    "\n",
    "当使用 cluster 或应用某些第三方包的时候，在 Executor 中会出现 ImportError 的错误，导致 job 执行失败，如下提交方式会报错：\n",
    "\n",
    "```\n",
    "spark-submit --master yarn \\\n",
    "             --deploy-mode cluster \\\n",
    "             code6.py\n",
    "```\n",
    "报错信息：\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"code6.py\", line 2, in <module>\n",
    "    import numpy as np\n",
    "ImportError: No module named numpy\n",
    "\n",
    "```\n",
    "创建虚拟环境 python-env，打包为 venv.zip：\n",
    "\n",
    "```\n",
    "virtualenv python-env\n",
    "```\n",
    "venv.zip 部分目录结构如下所示：\n",
    "\n",
    "```\n",
    "venv.zip\n",
    "└──python-env/\n",
    "    ├── bin\n",
    "    │   └── python\n",
    "    ├── include\n",
    "    ├── lib\n",
    "    └── lib64\n",
    "```\n",
    "spark-submit 命令：\n",
    "```\n",
    "spark-submit --master yarn  \\\n",
    "             --deploy-mode cluster \\\n",
    "             --archives ./venv.zip#env \\\n",
    "             --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=env/python-env/bin/python \\\n",
    "             code6.py\n",
    "```\n",
    "code6.py\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.appName('Test_Code1') \\\n",
    "                            .enableHiveSupport() \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "arr = np.array([1, 2, 3])\n",
    "print(arr)\n",
    "\n",
    "spark.sql(\"select count(*) from default.test_table\").show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
